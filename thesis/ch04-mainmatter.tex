\chapter{CBDRF调度系统的设计与实现}
\label{cha:frontmatter}
由于在集群调度时，尤其是在集群管理的第一层调度的时候，往往只考虑了资源的拆分，而对任务间的关联关系对虚拟机的调度尤其是虚拟机的摆放位置的要求往往很高，
而传统的资源调度系统在这方面往往考虑有所不足，本章以此为切入，详细的介绍了CBDRF调度系统的动机，设计，算法，和调度流程。

\section{CBDRF调度系统的提出}
云计算它采用创新的计算模式使用户通过互联网随时获得近乎无限的计算能力和丰富多样的信息服务，它创新的商业模式使用户对计算和服务可以取用自由、按量付费。目前的云计算融合了以虚拟化、服务管 理自动化和标准化为代表的大量革新技术。云计算借助虚拟化技术的伸缩性和灵活性，提高了资源利用率，简化了资源和服务的管理和维护；利用信息服务自动化技术，将资源封装为服务交付给用户，减少了数据中心的运营成本；利用标准化，方便了服务的开发和交付，缩短了客户服务的上线时间。而云计算的虚拟机资源调度器或是正调度算法是云计算平台的重要模块，是高效的解决资源融合，共享的重要组成。

目前主要有以下几种算法解决IaaS云计算平台的虚拟机资源调度策略的方法：
 \begin{enumerate}
\item 从平台的负载角度出发，当个任务或是应用发出虚拟机资源的请求时，平台检查所有物理机上的剩余资源，
将满足物理机请求的资源的物理机过滤出来，尽量使得各个物理机上的负载均衡。
\item 从平台的资源的利用的效率出发，当任务或是应用发出虚拟机资源请求时，检查所有物理家上的剩余资源，
采用最少的物理机来满足任务或是应用对资源的请求，使得任务平台整体的能够有较少的使用物理资源，减少不必要的资源浪费。
\item 基于公平性的调度算法，以DRF算法为例的公平调度算法，主要考虑各个资源分配直接的均衡和合理，
可以使得任务之间对资源的请求和分配达到经济学上的帕累托最优。
\end{enumerate}

而在目前的任务处理中，一般对虚拟机或是计算资源的请求
上述种方法在解决云计算中虚拟机资源调度问题的方法有各自的局限性：
\begin{enumerate}
\item 负载均衡算法为了保证各个物理机上的虚拟机负载尽可能一致，会导致虚拟机的分配方案相对比较松散，导致有大量交互或是大量网络通讯的任务或应用会或得比较糟糕的网络状况，导致服务质量的下降或是任务执行的时延增加。
\item 平台利用率的算法，尽量采用少量的物理机满足应用的虚拟机资源请求会或得比较好的资源利用情况，但是这种算法是一个典型的NP问题，求解这个问题往往会造成更高的调度代价，因此在实际的云计算平台中很少采用。
\item DRF算法本身在资源的利用情况会造成比较多的碎片情况，导致可分配，可调度的次数有所不足，另外由于忽略了本身物理机的位置的关系，会造成了物理机资源和任务的实际运行的网络情况不匹配的。
\end{enumerate}

综上，上述三种算法均在任务的通信请求和任务执行存在关联关系的问题上的考虑均有所不足，并且各自对资源的请求分配的情况均存在自己本身不足的因素。所以对大量通信的任务请求的调度结果往往不能取得较好的性能优势。

\subsection{任务的关联性}
在大规模的协同计算平台下，任务之间往往是存在关联性的。这种关联性主要有以下的三种关联，无关联的独立任务，DAG关联任务，和通信关联任务。

无关联的任务之间往往没有任何耦合关系，在计算任务时，这种任务往往是并行优化的重点作业，由于不存在任何的关联。
这些任务对资源的请求往往比较独立，没有任何的关联性，所以在资源的申请时往往不考虑通信和数据局部性的关系。

DAG关联任务是比较常见的协同作业之一，这种作业往往是把前一作业的输出作为下一作业的输入进行。这两个作业
耦合度比较浅，仅仅上一个任务结束和下一个相关的任务开始时有耦合关系。但是由于是一种特殊的类似拓扑关系
的耦合，导致如果上一作业没有进行完毕那么下一作业则不能开始执行，另外，由于存在一定的数据耦合，所以在大规模
集群计算的背景下，对数据的局部性要求是比较高的，如果花费大量的时间在中间输入和输出的传输和一致性同步上，将会
大幅的降低集群的计算性能，因此这种类型的作业极大的考虑数据的局部性。

通信关联任务是最为常见的协同作业。这种作业以MPI为例，分布式的进程互相之间首先需要大量的交换中间数据。交换的频繁
程度以用户的程序规定，一般会比较密集。并且这种作业的计算时间往往会比较长，以科研的高性能仿真实验为主。耗费大量的
CPU，内存，网络和硬盘数据。任务之间因为程序的不同，导致中间通信的代价往往不同。但是一旦存在互相之间的通信关系，那么
在DAG作业的基础之上，除了考虑数据的局部性之外，对于网络的拓扑情况也必须加以考虑。以MPI为例，同一局域网下的MPI的执行
效率至少是异地跨网段MPI执行效率的7倍，最差的时候多达35倍。

因此，在进行资源的调度的过程中，必须对资源的局部性和网络位置来考虑资源的分配，以往的资源分配往往仅仅将资源分配
抽象成一个切蛋糕或是划分资源池的过程远远不能满足框架的计算特性。好的资源切分可以使得框架的调度结果的性能有很大的
提升。

\subsection{系统的负载均衡}
云计算平台的一个重要的指标是计算的性能，以虚拟化为主的云计算平台上物理机的负载情况是影响虚拟机的计算性能的重要因素。
实践证明，当物理机上的建立的虚拟机的数量越多，物理机的负载越大，该宿主机上的虚拟机的效率下降越多。下图展示了KVM的物理机
上建立虚拟机的个数对物理机性能的曲线图。

从图中可以看出，在物理机上建立虚拟机个数比较少的情况下，虚拟机的性能基本保持不错，和相同配置的物理机的性能几乎相同。
而在此基础之上，如果继续加大虚拟机的个数，可以发现虚拟机的性能开始下降，但下降的趋势和虚拟机的个数基本成成正比关系。
但此时单个虚拟机性能的下降的损失可以被增加的虚拟机个数所弥补。当实际的负载进一步加大，虚拟机的性能此时急剧下降，增
加的虚拟机的个数完全不能弥补单个虚拟机性能的下降所造成的对整个集群啊性能的影响。

从此可以发现，单个物理机的负载必须在一个合适的范围内。在整个集群中，如果某个调度的策略不当，使得某个物理机的负载过高。
会使得该物理机上的运行的虚拟机的效率大大降低，从而使得在该虚拟机上实际运行的任务的效率大大降低，进而会影响与之有通信关联
或是DAG关联任务的执行，使得整个系统的执行效率降低。所以从资源调度的角度出发考虑，应当避免某个虚拟机的负载过高的情况出现。

针对系统的负载均衡，openstack的scheduler采用了filter－scheduler的策略，这个调度器的调度即就是采用基于负载均衡的角度设计的调度器
其调度的过程主要分一下两个决策过程。
\begin{itemize}
\item filter：这个过程是过滤的过程。按照用户的需求，过滤掉不符合调度策略的物理机。filter的策略配置有基于内存的过滤器ram\_filter，
基于vcpu的core\_filter等，作为云计算的调度器的调度器设计者或是平台维护者，也可以添加自己的filter的实现。
在调度的过程中，根据调度器开发者或是平台维护者的设置来选取不同的filter，可以选择其中的一个或者多着过滤器。多个
过滤器过滤的过程中对每个过滤器都进行一次过滤从而过滤出符合规则的调度器。
\item weight：当过滤过程结束之后将进入weight过程，这个weight的过程的实质是按照集权本身的负载情况进行排序，每次选取负载最小的
主机作为最终的调度结果。其中weigh的函数仍然可以由调度器设计者或是平台维护者自行定义，目前的openstack的调度器是采用的基于内存
的负载情况考虑，选择内存负载压力最小的主机作为最终的调度结果。
\end{itemize}

其他的基于负载均衡的调度策路还有，round-robin调度策路，best-fit策略，first-fit策略，这些策略都是基于当前云计算平台
的负载状态的所做出的调度策路。本质上使得每个物理机上的负载尽量相同，使得每台物理机的负载差异较小，不会出现单个物理机
负载过大，降低整体云计算平台整体性能下滑。

\subsection{主资源公平策略}
主资源公平（Dominant Resource Fairness）策略是UCBerkly提出的一种基于多资源的公平的资源分配，也是作为Apache Mesos的中央调度器所采用的
公平策略，其本质是最大最小公平的一个实现。在先前的云计算平台中大多采用的是基于单资源的公平分配，而DRF算法将这这种公平扩展向了多资源公平。

对于每个用户，DRF算法首先会计算分配给每个用户各个资源维度的分享量。而某个维度上分享量的中最大的值称为该需求的主分享量（domainit share）。
此时这个主分量的资源被叫做这个用户的主资源（domainint Resource）。对于不同类型的计算任务，其主资源的分配往往是不尽相同的，对于计算任务而言，
其主资源时CPU，而对缓存任务而言其主资源时内存而某些MPI计算任务其带宽则是其主资源，DRF算法的主旨是使得各个主分享量尽量相同。其算法的伪代码
算法5所示：
\begin{algorithm} 
\caption {DRF算法} 
\begin{codebox}
\Procname{$\proc{DRF allocation}()$}
\li	$R \leftarrow \left({r}_{1},...{r}_{m} \right)$ \RComment 所有的资源，共M种
\li	$C \leftarrow \left({c}_{1},...{c}_{m}\right)$ \RComment 已消费的资源，初始均为0
\li	${S}_{i} (1 \leq i \leq n)$				\RComment n个待分配的主分享向量，初始均为0
\li	${U}_{i} \leftarrow {{u}_{i,1},...{u}_{i,m}}(1 \leq i \leq n)$	\RComment 分配给i的资源数，初始为0
\li	$i \leftarrow {argmin}_{1 \leq i \leq n} {{S}_{i}}$ \RComment 选取主分享向量最小的
\li	${D}_{i} \leftarrow {{d}_{i},...{d}_{m}}$ \RComment 待分配的需求向量
\li	\If $C + {D}_{i} \leq R$
\li	\Then 
		$C \leftarrow C + D_{i}$                  
\li            	${U}_{i} \leftarrow {U}_{i} + {D}_{i}$ 
\li		${S}_{i} \leftarrow \max \limits_{1 \leq j \leq m} {{{u}_{i,j}}/{r}_{j}}$ \RComment 更新主分享向量
\li	\Else
\li		\Return
	\End
\end{codebox}
\end{algorithm} 

例如对于一个拥有<9CPU, 18GB>的系统，如果此时有两个任务A，B分别要求<1CPU，4GB>和<3CPU,1GB>内存的任务，按照，此时A任务的主资源为内存，B
任务的主资源为CPU，如果系统的资源要求尽可能全部分配，按照主资源公平算法5所述，进行5次调用，那么A任务将会分得<3CPU，12GB>内存，而B任务最终会被分配<6CPU，2GB>。
其分配的过程如下表所示
\begin{table}[htp]
\caption{DRF的分配示例}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
分配过程 & A的分配向量&A的主分享量&B的分配向量&B的主分享量\\
\hline
B & <0,0> & 0 & <3,1> & 1/3\\
\hline
A & <1,4> & 2/9 & <3,1> & 1/3\\
\hline
A & <2,8> & 4/9 & <3,1> &1/3\\
\hline
B & <2,8> & 4/9 & <6,2> & 2/3\\
\hline
A & <4,12> & 2/3 & <6,2> & 2/3\\
\hline
\end{tabular}
\end{center}
\label{default}
\end{table}

采用DRF算法的分配的各个向量之间的公平和负载时比较均衡的，DRF算法是很好的实现的资源负载的公平性
分配，当时DRF算法的一个重要问题如其算法的提出者所说，其仍将资源抽象成一个资源池的模型，没有考虑
资源分配的具体位置。对不同类型的框架任务来说，往往资源分配的主机位置，通信网络状态时影响计算性能
的重要因素。这一是限制DRF资源分配算法进一步提升的系统的空间。

\section{CBDRF调度的关键算法的设计}
CBDRF调度系统的核心是其调度算法，本节首先介绍了CBDRF算法的调度流程，接下来分别介绍其中的各个核心
调度模块，并给出调度逻辑的伪代码和流程图。
\subsection{CBDRF调度的流程}
CBDRF调度算法从解析用户请求开始，到最终的输出调度结果，主要经过一下9个步骤：
\begin{enumerate}
\item 首先，资源解析器解析用户请求，为每个请求初始化request实例。
\item 由关联任务的解析进行解析，生成带有特殊标记的任务序列。
\item request实例向资源调度迭代器发送解析数据，同时资源调度迭代器向资源管理控制器发送请求，以获取云计算平台的的实时资源状态。
\item 资源管理控制器为每个物理机创建host实例。
\item 将用户的请求作为要分配的资源池，当多个资源请求到来的时候将这些资源请求抽象为分配给物理机的总资源，
同时将物理机的容量作为实际的请求向量计算每个host(物理机)的dominantShare，
为资源向量所拥有的资源的占有率中的最大值称物理机户的dominantShare.
同时计算dominantDesire为资源向量上剩余最多的资源。
\item 由资源调度迭代排序器和资源调度迭代器运行基于DRF算法的分配，进行迭代的调度分配资源算法。
\item 资源调度迭代评估器利用Jain’s fairness index:简氏公平指数，计算资源请求request的dominantShare向量
$TS\left({ts}_{1},{ts}_{2},…{ts}_{k}\right )$，云计算平台物理机向量dominantShare向量HS，记录此时的$Value = \frac{J(TS)}{J(HS)}$和分配方案。
\item.资源调度迭代评估器计算$Value = \frac{J(TS)}{J(HS)}$，如果大于之前的最优值，记录此式的解决方案。
\item.资源调度迭代合并器从已经任务解析的任务序列中，按照关联任务的，从而更新合并mergeHistory跳至步骤5，否则跳至步骤11.
\item.算法结束，输出解决方案。
\end{enumerate}
\subsection{关联任务的解析}
关联任务的解析，是算法的预处理环节中关键的一步。这一步需要将对关联的任务的关联关系进行解析。
对于不同类型的任务，对资源的需求是不同的。需要对不同关联类型的任务进行不同的关联任务解析。
对于无关联的任务，不需要保证其数据的局部性和通信的交互，此时平台的物理机的任务位置均可以作为虚拟机开启的
的候选位置。而对于存在关联关系的任务DAG任务，和通信关联的任务则需要在调度的时候解析出这种关联关系。
在虚拟机调度的时候考虑数据的局部性。

对于DAG任务，优于任务之间存在比较强的耦合关系，在资源分配的过程中，可以将前置任务资源分配的结果保留作为后置资源分配的结果。
而前置资源的分配结果，可以取所有后置资源中最大的资源需求。在实际的云计算平台中，进行这么处理的将带来实际的两个优势：
\begin{enumerate}
\item 首先，如果将前置的资源分配的结果直接分配给后置的任务，那么这两个任务的资源只有一份实际的资源被分配。
提高了平台资源的利用情况。
\item 其次，由于对于任务而言，前置任务和后置任务同一个虚拟机或是计算节点中。将可以极大的减少任务之间中间
结果的传输过程，做到加速任务执行的过程。
\end{enumerate}

对于通信关联的任务。任务之间的通信关系往往是实时的，同步的。因此，无法将前置任务的资源直接分配给后置资源。
对于互相通信关联的任务，调度虚拟机的安放策略时，需要考虑的一个重要问题是保证通信的低时延特性。而通信的任务
往往是多个任务间同时进行通信的，在实际的调度任务计算过程中，需要将所有具有通信关系的任务尽量作为一个整体考虑
调度的结果中，尽量保证调度的虚拟机的安放是在同一物理机，如果不能保证，则应当保证其在同一机架上，其次是同一局域网内。
避免跨网段，跨机房的调度结果。

结合实际的作业中，任务之间的关系往往不是只有简单的DAG任务，通信关联或是无关联任务中的一种。任务的关联
关系往往是复杂的。这种关联关系由于这三种关系的组合，这种负杂的关系模型可以采用图论的模型加以描述和建模解决。
建模的规则如下所示：
\begin{enumerate}
\item 将任务抽象成图像中的点，将任务之间的通信关系抽象成图论中的有向边。
\item 对于无关联的任务，他们在图中则被描述成一个孤立的节点。
\item 对于DAG任务，如果任务B需要等待任务A的结束执行，那么在图中的关系则是一条从A指向B的有向边。
\item 对于互相关联的通信任务A和任务B，他们在图中的关系就是两条有向边加以描述，分别是A指向B的有向边和B指向A的有向边。
\end{enumerate}

将任务的运行和关联关系抽象完毕，借助图论的相关知识，同时结合资源调度分配的原则，我们进行任务
的分配时应当遵守以下规则：
\begin{enumerate}
\item 对于无关联的任务，可以作为一个对立的单元进行分配资源。
\item 对于DAG任务，应该将同一个DAG链上，以DAG链作为分配资源单元进行分配。
\item 对于互相关联的通信任务，在图论中对应的模型是图的联通分量。分配资源时，应当结合联通分量的子集，确保通信的代价不会成为系统
的瓶颈。
\end{enumerate}

对于任务的解析，必须按照上述的建模法则，分配法则进行处理，结合图论算法。我们应当按照下面的流程进行任务的解析过程。
\begin{enumerate}
\item 提取任务间的关联关系，将任务的关联关系描述成一个图的结构。
\item 按照图论的关系，计算强联通分量，标记各个任务属于哪个联通分量。
\item 将在各个联通分量提取出来，其分配序列按照资源请求数目由小到大进行排序。同时将得到的强联通分量看作一个整体的任务。
并将图的模型，结合原有的关系，得到一个新的任务关联关系。这是的任务关系是一个DAG任务图。
\item 将DAG任务图进行拓扑排序，得到新图的一个拓扑序列，在拓扑序列的每个结果中，
如果存在有联通分量缩点造成的新点，该点已步骤3得到的序列替换。
同时记录其DAG链上的资源需求的最大值，如果该链上存在联通分量缩点造成的新点，
该比较值去原序列的最大值。
\end{enumerate}
其算法的伪代码算法6所示。
\begin{algorithm} 
\caption {DRF算法} 
\begin{codebox}
\Procname{$\proc{DRF allocation}()$}
\li	$R \leftarrow \left({r}_{1},...{r}_{m} \right)$ \RComment 所有的资源，共M种
\li	$C \leftarrow \left({c}_{1},...{c}_{m}\right)$ \RComment 已消费的资源，初始均为0
\li	${S}_{i} (1 \leq i \leq n)$				\RComment n个待分配的主分享向量，初始均为0
\li	${U}_{i} \leftarrow {{u}_{i,1},...{u}_{i,m}}(1 \leq i \leq n)$	\RComment 分配给i的资源数，初始为0
\li	$i \leftarrow {argmin}_{1 \leq i \leq n} {{S}_{i}}$ \RComment 选取主分享向量最小的
\li	${D}_{i} \leftarrow {{d}_{i},...{d}_{m}}$ \RComment 待分配的需求向量
\li	\If $C + {D}_{i} \leq R$
\li	\Then 
		$C \leftarrow C + D_{i}$                  
\li            	${U}_{i} \leftarrow {U}_{i} + {D}_{i}$ 
\li		${S}_{i} \leftarrow \max \limits_{1 \leq j \leq m} {{{u}_{i,j}}/{r}_{j}}$ \RComment 更新主分享向量
\li	\Else
\li		\Return
	\End
\end{codebox}
\end{algorithm} 
\subsection{基于DRF算法的分配}
11111

\subsection{关联任务的合并}
11111

\section{CBDRF调度系统的组成}
在大规模的云计算平台下，云计算的资源调度在是云计算的平台的核心模块，本章从关联任务的角度出发。设计了一个基于关联任务
的中心调度器，设计并实现了基于关联关系的主资源调度算法。在该算法的基础之上，进一步的实现了基于关联关系的主资源调度器。
该调度器主要包含以下三个子模块，资源管理控制器，任务调度解析起喝资源调度迭代控制器。
\subsection{资源管理控制器}
在云计算平台中，资源调度管理器是资源调度器的一个必不可少的组件。资源调度管理器一般用于接受资源调度监控系统所获取的物理
宿主机的资源情况。同时维护和更新资源状态信息，保证资源状态信息的一致性。进一步，资源调度管理器除了完成与平台监控系统的
交互之外，在CBDRF中需要对资源监控所采取的数据进行预处理。从监控的数据中抽取调度算法所需要的数据，同时按照调度系统的需要
需要预先计算处理额外的请求已减少调度迭代器的计算，从而获得更好的计算性能。另外，由于云计算的平台存在实时并发的作业，资源
管理器必须作为一个长期服务的进程存在，能够实时获取整个平台的状态，为调度迭代器控制获得准确的数据。在此基础之上，还应当保证
并发控制的原子性和事务性。

综上所述，在CBDRF系统中资源管理器涉及的操作主要有以下：
\begin{enumerate}
\item 与资源监控接口对接，实时获取云计算集群资源状态情况。
\item 结合调度迭代控制器，从监控数据中抽取平台中各个物理机的资源情况，包括CPU，内存，带宽，硬盘的相应状态。
\item 根据云计算集群中每个物理机的资源情况，在调度迭代器控制器请求时，实时的计算出集群中的资源总量和剩余资源总量，
在CBDRF调度中，资源管理器还应当计算当前状态下各个物理机的dominantShare和doaminDesire。
\item 保证数据的一致性，保证集群状态信息的一致性，在并发环境下。在调度器迭代器高并发作业的情况下，提供对资源
的一致性，原子性和事务性。
\end{enumerate}
\subsection{任务调度解析器}
在面向计算任务的协同计算平台的背景下，单个任务往往不是只有一个单一的作业完成的。往往一个作业（Job）会有许多的任务（task）组成。
而不同的计算框架的task见的关系一般是不同的。MPI的计算任务往往是双向关联的通信关心任务，而Hadoop的任务大多以Map－Reduce等
DAG作业为主。而任务调度解析器必须能够解析不同框架提出的资源请求。

在CBDRF的调度系统中，任务解析器主要是重要的前端预处理流程和资源请求封装进行封装。由于调度迭代控制器需要对资源请求
进行相应的操作，任务调度解析器需要对不同框架的任务的资源请求进行操作，这就要求任务调度解析器必须封装资源请求的操作。
另外，对于关联任务的解析，任务调度解析器负责处理关联任务之间的解析，需要针对无关联任务，DAG任务，和通信任务设计相应
的算法进行解析，将最终的解析的结果以带有特殊标记的序列的形式提交给调度迭代控制器。具体来说，目前CBDRF调度系统主要
运行一下的功能。
\begin{enumerate}
\item 接受框架发来的任务调度请求用户的请求json文件，提取调度迭代器关心的CPU，内存等资源请求，同时进一步封装用户的资源请求。
\item 解析框架任务关联请求，运行强联通分量，拓扑排序等图论算法，产生带有特殊标记的任务序列。
\item 提供合并操作的封装操作，在利用并查集的操作上，提供后续的资源调度迭代器进行操作用以进行合并操作，
进一步资源调度迭代器能够查询合并历史产生资源调度结果。
\end{enumerate}

\subsection{调度迭代器}
调度迭代器是CBDRF调度系统的核心模块。资源调度迭代器主要进行调度的迭代控制过程，产生资源调度结果。
调度迭代器进一步包括资源调度迭代控制器，资源调度迭代合并器，资源调度迭代排序器，资源调度迭代评估器。
四个模块。

$\bullet$ 调度迭代控制器

资源调度迭代控制器是资源调度迭代器的重要控制元件，主要进行调度的迭代控制，是执行调度算法的重要工作
模块。其控调度度迭代合并器，资源调度调度迭代合并器以及资源调度排序器协同工作，判断迭代终止条件。
并且资源调度迭代控制器在迭代的迭代计算过程中，需要对资源请求进行分配，进行实际的分配的资源的计算。
资源调度迭代控制器涉及的主要操作有以下几个：
\begin{enumerate}
\item 接收资源请求解析器的请求，控制资源调度迭代器的初始化工作
\item 控制资源调度迭代排序器，根据dominantShare和dominantDesire进行物理机的评估排序。
\item 资源调度迭代器进行调度控制，根据贪心算法，采取调度控制过程，采用max－min方法，对dominantShare进行贪心分配。
\item 调用资源调度迭代合并器，控制解析出的同一连通分量的请求的进行合并，
\item 控制调度迭代器的调度迭代次数，并判断合并的结果是否能够进行下一轮迭代。
\item 迭代终止时，返回最终的调度结果。
\end{enumerate}

$\bullet$ 调度迭代合并器

调度迭代合并器的合并操作是调度迭代操作中的重要环节。用以产生中间迭代的调度中间结果。实际在调度的过程中，
调度迭代合并器负责按照CBDRF算法的设计进行合并请求的操作，按照CBDRF设计的算法，采用并查集的方法，根据
任务解析器发送的带有标记的任务序列进行合并操作。

$\bullet$ 调度迭代排序器

调度迭代排序器是调度迭代的操作中的辅助环节。这个排序器的主要作用是辅助调度迭代控制器进行分配的过程中。
为了能够对domainantShare进行贪心分配，必须对物理机和调度器分别排序，按照不同DomainantShare和DomainantDesire
进行排序，为了能够加速这个排序过程，调度迭代排序器对资源的合并操作采用了以堆为主要数据结构进行排序。
因为每次的迭代过程中，对物理机的操作的排序均是以原始状态进行，所以在排序器的维护中，保留了原始物理机上
资源信息状态的已排序副本，用以加速整个调度的进行。

$\bullet$  调度迭代评估器
调度迭代评估器是调度迭代器的最后一个环节，也是资源调度迭大器的一个重要模块。这个模块用以评估调度结果的公平性，
负载均衡性，等因素。支持多个评估函数的添加。在本此调度实现上，采用了基于简氏系数的公平评价指数。另外，
资源调度迭代评估器除了对调度结果的进行评，记录当前的评估结果之外，资源调度迭代评估器的
另一个重要作用同是与先前已经计算的迭代产生的调度中间结果进行比较，如果当前的评估值优于先前的
调度结果则记录调度结果为当前结果，否则，舍弃当前的结果，通知调度迭代控制器进行下一轮迭代器。

\section{CBDRF调度系统的实现}
\subsection{111112}
一种基于云计算平台下任务关联关系的虚拟机资源调度方案，用于产生虚拟机的调度分配策略，具体包含以下步骤
\begin{enumerate}
\item 首先，资源解析器解析用户请求，为每个请求初始化request实例，每个实例为五元组（cpu，mem，relatedRequest ,mergeHistory, scheduledHost）其中：
CPU表示任务请求的虚拟机的CPU核数。
mem表示任务请求的虚拟机内存大小，以GB为单位。
relatedRequest表示关联任务请求集合.
mergeHistory表示合并历史，调度迭代器结合scheduledHost从合并历史中解析最终的调度结果。
scheduledHost表示请求的调度结果，通过mergeHistory解析最终请求的调度结果。
上述CPU，mem，relatedRequest的初始值从用户的请求中获取，mergeHistory和scheduledHost初始集均为空集。

\item 由于关联任务之间互相存在网络通信，在同一个物理机上放置两台存在网络通信的虚拟机可以提升系统性能，并且可以提升资源的利用效率，因此将互相有交流的任务抽象成点，将他们之间网络通信关系抽象成边，通过relatedRequest将请求解析抽象成一个关系图。由资源解析器运行图论算法计算任务关系图的强联通分量。标记在request实例中。

\item request实例向资源调度迭代器发送解析数据，同时资源调度迭代器向资源管理控制器发送请求，以获取云计算平台的的实时资源状态。

\item 资源管理控制器为每个物理机创建host实例，每个实例为四元组（cpuTotal,cpuUsed, memTotal, memUsed）其中：

cpu表示分别单个物理机表示cpu核数。

cpuUsed表示单个物理机已经使用的cpu核数。

memTotal表示单个物理机内存总数，以GB为单位。

memUesd表示单个物理已经使用内存总数，以GB为单位。

上述数值均通过云计算平台的监控接口获取这些数值。

\item 将用户的请求作为要分配的资源池，当多个资源请求到来的时候将这些资源请求抽象为分配给物理机的总资源，同时将物理机的容量作为实际的请求向量计算每个host(物理机)的dominantShare，为资源向量所拥有的资源的占有率中的最大值称物理机户的dominantShare.同时计算dominantDesire为资源向量上剩余最多的资源。其定义如下所示：

云计算资源总数向量表示如下所示：
\begin{equation}
R=\left({r}_{1},…,{r}_{m}\right)
\end{equation}
单个物理机i资源总量的向量表示如下所示：
\begin{equation}
{HT}_{i}=\left({ht}_{i,1},…{ht}_{i,m} \right)
\end{equation}
单个物理机i已分配资源总量：
\begin{equation}
{HU}_{i}=\left({hu}_{i,1},…{hu}_{i,m}\right)
\end{equation}
单个物理机i的dominantShare计算方法：
\begin{equation}
{HS}_{i} = \max \limits_{1 \leq j \leq m} {hu}_{i,j}/{{r}_{j}} 
\end{equation}
单个物理机i的dominantDesire计算方法:
\begin{equation}
{HD}_{i}=\max \limits_{1 \leq j \leq m } {ht}_{i,j}-{hu}_{i,j} 
\end{equation}
\item 资源迭代合并器计算request的dominantShare以及dominantDesire，并按照dominantShare的大小由小到大的排序，相同的dominantShare则按照doaminantDesire的大小由小到大进行排序。其中request的dominantShare和dominantDesire如下所述：
任务请求资源总数向量：
\begin{equation}
T={t}_{1},…{t}_{m}
\end{equation}

任务请求单个物理机i请求资源总量：
\begin{equation}
{TR}_{i}={tr}_{i,1},…{tr}_{i,m}
\end{equation}
任务请求物理机i的dominantShare计算方法：
\begin{equation}
{TS}_{i}=\max \limits_{1 \leq j \leq m} {{tr}_{i,j}}/{{t}_{j}}
\end{equation}
任务请求单个物理机i的dominantDesire计算方法
\begin{equation}
{TD}_{i}=\max \limits_{1 \leq j \leq m} {tr}_{i,j}
\end{equation}

\item 由资源调度迭代排序器和资源调度迭代器运算贪心算法，进行实际的分配操作，分配算法如下伪码所示：
\item 资源调度迭代评估器利用Jain’s fairness index:简氏公平指数，计算资源请求request的dominantShare向量
$TS\left({ts}_{1},{ts}_{2},…{ts}_{k}\right )$，云计算平台物理机向量dominantShare向量HS，记录此时的$Value = \frac{J(TS)}{J(HS)}$和分配方案。其中简氏公平指数的计算方法如下所述：
\item.资源调度迭代评估器计算$Value = \frac{J(TS)}{J(HS)}$，如果大于之前的最优值，记录此式的解决方案。
\item.资源调度迭代合并器从已经计算的任务强连通分量重中选取排序好的request中按照哈夫曼合并法则，选取最小的2个request进行请求合并，如果此时的任务关系图成为一个有向无环图，
则在此有向无环图上进行拓扑排序，选取拓扑排序节点上相邻最小的request进行合并，从而更新合并mergeHistory跳至步骤5，否则跳至步骤11.
\item.算法结束，输出解决方案。
\end{enumerate}

\subsection{111113}
\subsection{111114}
\section{本章小结}
11111