
\chapter{云计算资源管理与调度相关技术}
\label{chap:outline}

对于企业和公司，为了完成各种对外的服务以及公司内部业务逻辑，需要大量的硬件资源, 而硬件的资源的代价往往比较昂
贵，所以如何充分挖掘硬件资源潜力从而增加硬件的利用率

\section{云计算平台}
\subsection{OpenStack云计算平台}

OpenStack是最初是NASA和Rackspace的开发云计算平台，该云计算平台以Apache Lincese开源协议发布，是一个自由的开源项目。
OpenStack与传统的软件不通，本质上来说不是具体某个软件，这个项目是由若干个子项目组成，这几个子项目一起构成了整个Openstack项目。
\begin{figure}[htbp]
\centering\includegraphics[width=0.8\textwidth]{Openstack-crop}
\caption{Openstack的组件关系图}\label{fig:Openstack}
\end{figure}
OpenStack是一个开放的云计算开源项目,该项目的目的是为公有云和企业私有云提供开放的管理平台。
目前全球大约有130家互联网企业和软件企业和1500多名自由软件开发者加入了维护Openstack的技术社区中。
作为一个典型的基础设施即服务的平台，这些企业和个人均以此作为其计算平台的底层进行维护和开发。
作为OpenStack的开发而言，该项目的重要功能是给软件开发带来简洁的云计算设施的部署方案，提供良好的可扩展性和可维护性。
其中，OpenStack的子项目有很多，器框架如图\ref{fig:Openstack}所示，但是其核心的子项目主要有以下9个：
\begin{itemize}
\item 计算(Compute)：Nova，Nova是OpenStack云中的计算组织控制器 。支持OpenStack云中实例（instances）生
命周期的所有活动都由Nova处理。Nova的核心模块有Nova-api,nova-conductor,nova-scheduler等。
其中的nova-scheduler作为nova的核心模块，该模块会根据云计算平台的负载，CPU，内存，可用区域以及网络拓扑等因素，
根据用户的需求，采用合适的算法，从云计算平台中调度决定资源的分配决定。
\item 对象存储(Object)：Swift，Swift是Openstack的分布式对象存储服务，最初是由RackSapce开发的，后来Racksapce讲该项目
开源并贡献给Openstack社区并作为整个Openstack社区的核心子项目之一。Swift项目的开始的主要作用是作为glance服务的存储后端，存储
虚拟机的镜像服务，现在也可以作为单独的存储服务提供给用户。而swift作为分布式对象存储服务，由于其松散的耦合设计，
以及其支持多租户，支持对容器和对象的读写操作，因此很适合解决当前互联网大数据背景下
的非结构化数据的存储问题。
\item 镜像(Image)：Glance,作为Openstack中的镜像管理服务存在。主要负责维护镜像存储的映射关系。在OpenStack中，glance不提供任何的存储服务。
他的主要作为是为读写多种后台存储镜像的后端服务提供一种抽象的接口。因此，glance还负责保存镜像的元数据态信息。
\item 身份(Identity)：Keystone(OpenStack Identity Service)是OpenStack框架中，负责身份验证、服务规则和服务令牌的功能， 它实现了OpenStack的Identity API。
\item 网络地址管理(Network)：Neutron是OpenStack核心项目之一，作为SDN网络的典型代表，负责整个Openstack平台的虚拟机的虚拟化网络。
\item 块存储(Block)：Cinder，与swift服务不同，Cinder为整个云计算平台提供可可持久话的块存储服务。Cinder为整个
平台中提供了块存储单元块的管理服务。在OpenStack中，建立的虚拟机的数据内容需要通过建立和挂载volume来实现数据持久化，
而volume实现的底层是由Cinder提供的，否则该虚拟机是不能提供持久化服务的。
\item UI管理界面(Dashboard):Horizon,Horizon基于python的Django实现，给云计算平台的服务商提供了一套可视化的webUI，能够
使得服务提供商实时观测云计算平台的状态，负载和规模。并且能够在该UI上统一部署和使用Openstack平台上的服务。
\item 测量(Metering)：Ceilometer,主要负责对虚拟机的监控工作。Ceilometer除了主要实时采集虚拟机的CPU，内存等实时性能指标外，
还会检测neutron-l3-router使用的网络带宽情况以及各个租户在cinder，glance以及swift上使用的存储信息。
甚至还会通过snmp来采集云计算平台宿主物理机的性能指标和支持opendaylight的网络设备信息。
\item 编配(Orchestration)：Heat与Amazon AWS的CloudFormation类似,
heat提供了一种高效的简洁的集群部署方案，通过heat可以采用简洁的配置和资源需求实现集群的快速部署工作。
用户可以在heat中定义yaml模版，这些模版定义了任务的资源请求和用户对任务执行的先后次序，使得可以自动部署计算集群任务的功能。
除此之外，利用heat可以在neutron上进行负载均衡的编排以及使用其他的网络附加功能。
\end{itemize}

\subsection{Mesos}

如图\ref{fig:mesos}，Apache Mesos由四个组件组成，分别是Master，Slave，Framework和executor。
\begin{figure}[htbp]
\centering\includegraphics[width=0.8\textwidth]{Mesos-crop}
\caption{Mesos的组件关系图}\label{fig:mesos}
\end{figure}

Master是Apache Mesos的核心组件，其负责管理Mesos集群中的各个framework和salve，其中frame\_manager管理
接入平台的各个framework而slaves\_manager则负责管理各个接入的slave节点。最后Master中的Allocator负责管理
各个Slave上的资源，并将其资源抽象整合为资源池的形式分配给各个Framwork。在这里由于Master节点存在单点故障的可能，
因此，Mesos采用了Zookeeper来解决Master的单点故障。

Slave是Apache Mesos的重要组成。Slave负责管理实际的物理节点，接受来自Master的指令，负责管理和监控分配在给节点上的任务，
同时管理该节点的资源情况，为分配在该节点上的任务分配所需的资源。当一个任务结束释放资源或者是新接入一个新的Slave节点时，
由Slave将该节点的资源情况汇报给Master。而在Master中由Allocator组件负责分配资源给各个接入集群的Framework。当一个用户在平台
上提交任务时，该任务的Framework会向Master提供由用户发出的资源请求，Allocator会将平台中某个Salve中的资源分配给该任务，在这个Salve上
会启动具有相应资源的Linux Container来运行相应的计算任务。任务间的的资源隔离就由该Salve上的Container进行负责。

Framework是指运行在该集群上的计算框架，典型的代表是Hadoop和MPI等。为了Mesos能够统一的管理整个集群的资源情况，要求这些计算框架必须通过注册的方式接入平台之中。同时
Mesos要求这些计算框架必须有一个任务调度器来进行内部的资源分配和任务调度\cite{ref15}，同时，对于Hadoop，MPI等集群，必须修改原始的调度器。在原有的调度器基础之上，添加与Mesos交
互的注册接口。同时，计算框架还应实现相应的接受和请求资源接口以接受Mesos分配给框架的资源。在Mesos中，调度的层级变成了2层，首先是由Mesos将资源分配个各个框架，而后在
各个计算框架中，进行第二层的调度，这层调度进行任务的资源分配和任务执行顺序的分配。Mesos本身提供了C++，Java和Python实现的调度器的抽象类，方便各个框架的调度器的接
入。而在内部则采用C++实现了一个调度驱动器，各个框架通过继承的调度器与该驱动器的接口进行交互，完成框架的注册以及Framwork间资源分配的任务。

Executor是实际启动框架的执行者。在各个Salve上负责启动各个框架的任务。不同计算框架间启动一个新的任务的方式往往是不同的，为了能够统一的执行不同框架的任务。Mesos要求接
入Mesos的框架的必须实现该框架的Executor，与计算框架的调度器相类似，Mesos提供了该Executor的抽象类，使得各个计算框架能够编写统一的执行逻辑。在Mesos内部则通过一个执
行驱动器实现与该继承的Executor的交互任务的功能。

\subsection{YARN}

YARN是Hadoop2.X版本的。YARN的主要作用是解决集群的管理问题，与Hadoop不同，YARN是一个通用的计算资源管理管理平台而非单一的计算框架。YARN的框架如图\ref{fig:Yarn}整个系统主要由三个组件
ResourceManager，ApplicationMaster和NodeManager组成。与Apache Mesos相类似，YARN也是一个两层的调度系统，全局的资源分配由ResourceManager分配给各个Application
而Application就类似于Mesos中的Framework，可以是Hadoop或是Spark等。进一步，各个Application由ApplicationMaster进行再一次的任务资源分配，最终告知各个物理节点上的
NodeManager，由各个物理节点上的NodeManager启动相应的Container运行相应的计算任务。
\begin{figure}[htbp]
\centering\includegraphics[width=0.8\textwidth]{Yarn-crop}
\caption{YARN的组件关系图}\label{fig:Yarn}
\end{figure}

$\bullet$ ResourceManager

ResourceManager的核心组件有Scheduler和ApplicationsManager

YARN作为一个两层调度策略的典型架构，其Scheduler的作用主要有负责与各个运行在YARN上的ApplicationMaster进行通行，负责向ApplicationMaster之间进行资源分配的工作，与
Mesos类似，Scheduler并不对Application的任务进行调度和监控也不负责相应的容错机制等。Scheduler只是负责分配container上的资源给各个ApplicationMaster，区别于
Mesos的Container，这里的Container是指各个资源抽象的最小单元而不是Linux Container。

ApplicationsManager主要作用有两个，首先是当用户提交第一个该类型任务的申请时，创建相应的Container运行该Application的ApplicationMaster，第二个作用当该applicationMaster心跳
包同步失败的时候，重新启动applicationMaster。
ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。

$\bullet$ NodeManager

NodeManager于Mesos的Slave相类似，NodeManager主要负责启动ResourceManager分配给ApplicationMaster运行作业的Container和汇报物理节点的资源状态。主要负责物理机上的上
的初始化工作，在计算任务开始前，负责下载在该Container上所需的数据文件，以及设置相应的环境变量和初始化相应的运行环境等。当所有的准备工作运行完毕之后，NodeManager讲
启动Container上的Application。并且NodeManager还会周期报告Container的运行情况，当Container上运行的作业完成或是超出资源被终止时，将释放的资源汇报给ResourceManager进
供其进行调度。

同时，NodeManager还会周期性的汇报给ResourceManager的改节点的工作状态。

$\bullet$ ApplicationMaster

ApplicationMaster与Mesos中类似的是各个框架的Scheduler，与Mesos相似，如果某个计算框架需要在YARN上部署，必须要实现相应的ApplicationMaster。ApplicationMaster的首先周期性的会向ApplicationMaster发送心跳包来汇报自己当前的工作状态。另外Application的重要左右便是向ResourceManager提出资源请求，当ResourceManager告知分配给其相应的Container时，向被分配的Container发送相应的控制信息和该计算任务的输入数据等该Container上的NodeManager。

\section{资源管理抽象模型}
 
资源管理调度模型如图\ref{fig:SchedModel}所示
\begin{figure}[htbp]
\centering\includegraphics[width=0.8\textwidth]{SchedModel-crop}
\caption{资源管理与系统的抽象模型}\label{fig:SchedModel}
\end{figure}
图中说明了一个资源管理与系统的抽象模型。从概念上讲资源管理与调度系统的主要目的是根据用户任务的请求，
从集群中分配资源。目前资源分配的主要资源主要包括内存,CPU,网络和IO资源等.而资源调度的模型主要涉及3个要素
资源组织模型,调度算法和任务组织方式。
\begin{enumerate}
\item 资源组织模型主要是整理和规整集群中所有的资源，以方便后续的资源分配过程。通常的做法是将资源组织成为多层级队列的形式，
例如Corona的"All-resource->group->pool"的三级队列模式组织，另外平级队列或是单队列也是常见的资源
组织模型，其本质是多层级队列的特殊形式。
\item 资源调度算法的负责将集群中的资源按照一定的策略分配的任务。常见的调度策略有FIFO调度算法、公平调度算法、能力调度算法以及延迟调度算法等。具体策略可以参加本章2.4资源调度算法。
\item 任务组织方式主要是将多用户提交的任务通过一定的方式组织起来，来方便调度算法进行分配。常见的组织方式是将任务以队列的形式组织起来，例如Hadoop1.0中将任务按照多队列组织，队列之间采用平级关系，而在hadoop2.0中的任务队列则增加了层级队列的树形结构，用以提供更加灵活的方式来管理任务队列。
\end{enumerate}
\section{资源管理与调度系统范型}
目前有多种多种的资源管理和调度系统实现，根据实际的宏观运行机制进行分类，可以分为以下几种资源管理与调度系统范型:集中式调度，两级调度器和状态共享调度器。

\subsection{集中式调度器}
集中式调度器在整个系统中只有一个唯一的全局调度器，其特点是，资源的调度和作业的管理功能全部放到一个进程中完成，在
集群上运行的所有框架或者是计算任务的资源请求均由集中式调度器来满足，因此，整个调度系统在集群上缺乏并发性，并且所有
的调度逻辑均由集中式调度器独自完成。开源界典型的代表是Hadoop JobTracker的实现。

这类调度器又分为2种类型，一种是单路径调度器,另一种是多路径调度器。
\begin{enumerate}
\item 单路径调度器(Single Path Scheduler):这种调度器是指不管任务的类型均采用全局统一的调度器进行资源管理，
这种调度算法基本是采用融合考虑各种因素来实现的调度器,在此基础之上结合任务的优先级，集群资源的状态统计，决定调度的
任务资源分配和调度顺序。
\item 多路径调度器(Multi Path scheduler):这种调度器在单路经调度器的基础之上做了该进,首先支持多种调度策略,其大致思路
是将调度算法模块化,根据任务的类型进行调度,比如批处理的任务采用批处理的调度算法,计算量大的任务采用计算量大的调度
算法,在具体的实现的时候各个算法独自实现,而在调度器的逻辑中根据任务类型进行选择,其选择类型类似于结构化程序编程的switch
-case分支路径,这种调度器可以根据调度算法的类型采用多线程的方式进行实现,较单路经调度器的调度灵活性和
并发性有了一定的提高。
\end{enumerate}

但是集中式调度器的设计方式的缺点也是比较明显的。
\begin{enumerate}
\item 由于将所有的调度逻辑均写入中央调度器中，所以实现的逻辑比较负载，可扩展较差。这点在单路径调度器上尤其明显，多
路径调度器虽然在此基础之上进行了改进，但是在针对某个类型任务的算法进行替换的过程中，中央调度器此时不得不整体停止，
对其他的调度任务产生影响。
\item 集群的规模受限，中央调度器的并发性不足.由于是对整个系统的全局资源进行调度，在小规模集群上可以完成其调度工
作而当集群规模扩大时，任务和工作负载加大，当调度与资源分配决策执行时间较长的情况下，这个时候往往调度器会首先达到
工作饱和，此时，后续任务会花费大量的时间在等待被调度，从而导致集中式调度器成为整个系统的运行瓶颈，严重影响系统的运
行性能。
\end{enumerate}

\subsection{两层调度器}
当集群规模扩大时，为了解决单一调度器的不足。在这个基础上开发者开发了两层调度器。两层调度器将整个系统的调度工作分成两个级别：
中央调度器和框架调度器。中央调度器负责集群系统中所有资源的管理，并按照一定的策略将集群中所有的资源分配给各个计算
框架，从总体上看，中央调度器进行的是一种比较粗粒度的资源调度，各个计算框架在接受到中央调度器资源的分配后，根据自身计算任务的特点
再次按照框架调度器调度进行任务执行的调度。在这种情况下，中央调度器进行资源的切分，而框架调度器重点则在任务的调度
上，框架调度器进一步细粒度的使用中央调度器所划分的资源，本质上是分而治之策略或者是策略下放机制。Mesos，YARN是
典型的两层调度的实现。

与集中式调度器相比，双层调度器由于存在框架调度器，各个框架调度器仅仅与中央调度器进行耦合，所以具有良好的并发性，系
统的整体性能较佳，也比较适合大规模的多任务高负载的计算任务，
但是双层调度器的缺点也是比较明显的，其主要问题有以下两个
\begin{enumerate}
\item 各个计算框架并不知道整个集群的工作状态。由于采用的静态的切分方案，整个物理机集群被人为的切分成若干
个不同的工作集群，这些集群之间互相并不知道各自的工作状态。而当一个长作业的计算框架占用了大量的计算资源
而此时的长作业主要在进行大量的I/O操作而并未释放集群啊时，将导致其他计算框架无法获知该框架上有大量的可以
复用的计算资源，从而导致整个平台的工作性能下降，使得大量的作业被等待或是拖延，这对于短作业的在线任务往往
是不可忍受的。
\item 资源划分采用悲观锁的机制，使得并发的粒度不足。在Mesos中，调度器在调度的过程中采用的全或无的关系，在
调度的过程中，会将平台中剩余的资源调度给一个集群而不管实际集群的使用需求，而当资源没有释放时往往是不进行调度
的，这样的过程相必须在资源的使用上采用了全局锁的机制。使得框架间无法去实时的竞争自己所需的资源，尤其是对
短时间作业和长时间作业并存的平台中，这种机制将导致大量的短作业被等待。另外，由于分配采用了锁的机制，导致并发
分配的性能显然不如多版本并发控制的机制的并发性好。
\end{enumerate}
\subsection{状态共享调度器}
状态共享调度器是Google的Omega\cite{ref33}调度系统提出的一种资源管理与调度范型\cite{ref38}。在这种调度范型里面，各个计算框架可以
 看到整个集群的所有的资源的状态。集群之间采用互相竞争的方式去获得自己所需的资源，根据自身特性采取不同的调度
 策略。为了能够克服双层调度器的缺点，状态共享调度器做了如下的工作，首先中央的调度其维护一个中央调度的共享数据，
 而在各个框架之间通过竞争的手段来验证这些数据的可访问性，而这里的“共享数据”实际上就是整个集群的实时资源使用信息。一旦引入共
 享数据后，共享数据的并发访问方式就成为该系统设计的核心，而Omega则采用了乐观锁的多版本并发控制来提升整个系统
 在调度的并发性。
 
与两层调度器不同呢，在状态共享调度器的调度框架中，由于中央调度器不进行实际的分配工作，在调度的过程中不对资源
按照框架进行切分，同时也不对各个框架的资源的使用进行限制，在状态共享调度器的架构下，各个中央调度器仅
仅维护一本可恢复的集群资源状态的主信息副本。而每个计算框架维护一份私有的副本信息，这些副本的信息由各个框架
调度器自我管理和控制，框架对系统的资源需求和直接在该副本上进行；只要框架的优先级足够，就可以在该副本上申请
相应的资源获释抢夺已经分配给其他计算框架的资源，一旦框架做出决定，则该副本的资源信息将通过事务的方式同步到
全局的状态共享副本中去，从而保障操作的原子性。

由于采用了MVCC的并发访问控制策略，并且支持优先级的调度过程，框架之间会产生冲突和竞争，在多个框架申请同一
份资源时，优先级最高的框架会获得该资源，而这个资源的具体分配就由该框架的调度器进行调度。这个过程可以看出，Omega
是一种以效率为优先的调度器设计，由于高优先级的抢占的存在，往往使得低优先级的任务得不到满足，可能会产生饿死的
可能。从此可以看出，如果资源访问冲突的次数越多，则在协调冲突的代价越大，造成调度的开销变大，影响实际的平台性能，
而google通过实际负载测试证明，在现实中往往在线的计算任务往往较多，并且实时性要求较高，具有较高的优先级，而后台的这种批处理，统计的作业较少
，两种作业的异质性较高，因而这种冲突的方式和冲突次数是完全可以接受的。

上述简述了3种不同的资源管理与调度范型，从集中式调度器到两层调度器再到状态共享状态调度器的发展过程来看，这是
一个逐步弱化中央调度器，逐渐增强框架调度器的过程。在提高调度并发性的同时，我们可以看出不同调度范型适合的应用
场景启示并不尽相同。集中式调度器比较适合小规模集群下的资源管理，而两层调度器比较适合负载同质的大规模集群应用
场景，而状态共享调度器则比较适合异质性并且冲突不高的大规模集群应用。

另外，从不同调度范型的发胀过程可以总结出，大规模数据处理系统的一个趋势是在硬件层上建立一个统一的资源管理调度
系统，而这个资源调度管理系统应该是由一个提供弱服务的中央的中央调度器和多个自由度较高的框架调度器组成，以增强
集群的硬件利用率，集群调度的并发性，可扩展性和调度的灵活性。

\section{资源调度算法}
 在多用户作业的环境下，无论中央调度器还是框架调度器都要涉及资源分配和任务分发的策略和算法。本节主要讨论常见的
 资源调度算法和任务分配策略，其中Hadoop已经将FIFO，公平调度器和能力调度器集中进去，另外，本节还将介绍一种常
 见的优化任务策略。
\subsection{FIFO调度算法}
FIFO策略是最简单的调度策略，提交的作业按照提交时间的先后顺序获释根据优先级将其放入线性队列之中，在资源调度的时候
按照次序取一个或者多个按照先进先出的顺训进行资源进行调度。这种调度算法比较常见于任务队列或是集中式调度的器的实现，
由于整个集群中仅仅存在一个队列，按照队列先后顺训进行资源分配，当集群的规模较大，这个调度器对新提交的作业的处理往往
不是及时的，会造成短作业的任务的饿死的情况。
\subsection{公平调度策略}
公平调度的策略是FaceBook为Hadoop开发的调度器，重点是使得各个作业之间能够按照时间片平等的分享整个计算平台的计算资源。
如果整个计算平台上只有一个作业在运行的时候，他将独占整个计算平台的资源。如果此时有新的作业被用户提交，那么平台会分配资源给这些新的作业，
使得来那个作业能够平等的分享平台的计算资源。与FIFO调度算法不同，这个算法不会使得后提交的短时间作业等待时延过长，同时如果有
优先级较低的长时间的批处理作业，这个策略也不会使得短作业独占平台，使得长作业被饿死的情况出现。同时，在集群中也可以通过配置
作业的优先级权重进行分配，使得各个资源的分配尽量合理。

在公平调度器中，资源仍是以资源池的形式来组织作业的资源模型的，并且按照公平策略将资源分配到各个作业的资源池冲。
在一般情况下，对于每一个用户，该用户均拥有一个独立的资源池，
这样的好处使得各个用户都可以平等的获得一份平台的资源，在这个资源池中，用户可以提交更多的作业在这个资源池中进行计算
任务。

同时，在公平的分配任务中，公平的调度器会保证各个资源池的最小的资源保证量。这个保证量在调度的过程中，会被做到基本的保证。
这个过程使得用户的作业任务可以在计算的过程中，可以保证其性能不至于计算的过程中下降。当平台的资源剩余量过剩时，将会在保障
最小保障量的基础之上，当一个资源池包含作业时，它至少能获取到它的最小共享资源，但是当资源池不完全需要它所拥有的保证共享资源时，额外的部分会在其它资源池间进行切分。

公平调度器作为FaceBook为Hadoop1.0的实现中已经在已经被分为两层，已初步具有两层
调度的雏形。第一层将集群中的资源平等的分配给各个用户所对应的资源池中，第二层则是在各个资源池中为各个作业公平的分配资源。
其调度的过程大致为以下三步：
\begin{enumerate}
\item 根据每个资源池的最小资源保障量，将系统中的部分资源分配给各个资源池。
\item 将平台的剩余资源根据资源池的优先级按照比例分配给各个资源池。
\item 各个资源池中的作业，按照优先级组织作业的队列。
\end{enumerate}
　　
当用户提交一个新的作业时，公平调度器会等待正在运行的任务运行完毕，当该任务运行完毕后会释放相应的资源给新提交的任务。
同时，公平调度器是支持抢占操作的，当一个新提交的作业的等待时间超过了系统所设定的等待阈值后，
这个新的作业仍然不能获得最小的资源保证量。这个作业可以请求调度器可以请求调度去去终止在运行的优先级较低的任务以获取相应的资源。
在Hadoop中，作业失败会导致任务丢失，会使得整个任务的运行时间变得更长，因此，为了减少用户的等待时间，公平调度器
在选择杀死的任务的时候，公平调度器首先考虑的是最近最新被运行起来的作业。这样的对集群的代价往往比较小。
\subsection{能力调度器}
可以看作是FifoScheduler的多队列版本。在各个任务队列中，可以限制资源申请情况。
而在能力调度器中，队列间的资源分配是以资源使用量由小到大进行排序，这样使得容量较小的队列往往可以获得较高的优先级。
而这些任务往往是短作业任务，这样使得集群的整体吞吐量变大。另外，能力调度器也实现了延迟调度策略，
使得用户的任务会优先考虑本地调度，在本地调度不被满足的情况下在考虑跨机器，跨机架的调度。。

在能力调度器中，各个队列是资源分配的单元。在每个任务队列中，这个队列可以指定该队列使用资源的最小下限和最大上限。
同时用户也可以在任务中制定任务需求的上下限。在作业执行的过程中，如果某个队列的资源存在剩余，
该列的剩余资源可以经公平调度器共享给另一个队列，当被共享的资源的队列执行完毕后需归还相应的资源。
同时在运行的过程中，平台集群的管理者可以对队列，作业和用户的资源使用进行限制。同时，在运行的过程中
可以分配给某些任务多个slot，但是区别于公平调度器，能力调度器支持作业优先级但是不提供资源的抢占机制。
这是因为能力调度器强调的是多用户在资源上的公平而非计算资源在作业上的公平，因此牺牲多作业间公平来保证用户间分享公平。
\subsection{延迟调度策略}
延迟调度严格的来说并不是一个调度算法，延迟调度策略\cite{ref37}往往会作为其他策略增加调度的数据局部性，以此来增加任务的执行效率。

其基本的的思想如下所示：
\begin{itemize}
\item 对于当前被调度的要被分配的任务，如果当前的资源不满足数据的局部性，那么可以暂时放弃分配公平性，任务不接受当前资源，而是等待后续的资源分配,
当前资源可以跳过该任务分配给其他待调度的任务.
\item 如果该任务被跳过的次数超过阈值扔无法满足数据的局部性，那么放弃数据局部性，接受当前的资源运行该任务。
\end{itemize}
延迟调度的很典型的应用在Hadoop的应用上。在Hadoop中，任务的数据均是存储在HDFS中，用户
提交的输入数据和任务运行过程中迭代产生的输出数据也是存储在HDFS中。一个作业在运行的过程
中，HDFS会讲数据拆分成若干份，在调度的过程中，尽量将任务调度掉拥有这些数据副本的物理机
上，这个就是延迟调度的主要目的，减少中间的数据传输的代价来保证数据的局部性。

以具体的例子如果某个输入文件在A，B，C这三台物理机上进行存储。当任务进行调度时，首先
看这三台机器能否被选作调度的物理机，因为这样可以避免通过网络传输数据。此时，如果不满足
被调度的条件那么调度器将放弃过此次调度，等待后续的分配。如果超过等待次数阈值，则放弃局部性接受当前结果。

在YARN的实现方式则是在ApplicationMaster给ResourceManager提交资源申请的同时，同时发送相应的数据局部性的要求。
而在resourceManager进行调度的时候，按照其局部性的要求，首先考虑匹配本地数据请求，接下来匹配同一机架的申请，最后
才是做整体的全局申请。

\section{本章小结}
本章主要详细的介绍了云计算中的平台，云计算资源调度器的设计思路，设计范型等，同时给出了常见的资源调度的算法和策略。
