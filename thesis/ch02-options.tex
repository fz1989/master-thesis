
\chapter{云计算资源管理与调度相关技术}
\label{chap:outline}

对于企业和公司，为了完成各种对外的服务以及公司内部业务逻辑，需要大量的硬件资源, 而硬件的资源的代价往往比较昂
贵，所以如何充分挖掘硬件资源潜力从而增加硬件的利用率

\section{云计算平台}
\subsection{OpenStack云计算平台}

OpenStack是一个美国国家航空航天局和Rackspace合作研发的，以Apache许可证授权，并且是一个自由软件和开放源代码
项目。

OpenStack是一个云平台管理的项目，它不是一个软件。这个项目由几个主要的组件组合起来完成一些具体的工作。

OpenStack是一个旨在为公共及私有云的建设与管理提供软件的开源项目。它的社区拥有超过130家企业及1350位开发者，这
些机构与个人都将OpenStack作为基础设施即服务（简称IaaS）资源的通用前端。OpenStack项目的首要任务是简化云的部
署过程并为其带来良好的可扩展性。OpenStack的核心组件有以下9个：
\begin{itemize}
\item 计算(Compute):Nova，Nova是OpenStack云中的计算组织控制器。支持OpenStack云中实例（instances）生
命周期的所有活动都由Nova处理。其中的调度器nova-schedule作为一个守护进程运行，通过恰当的调度算法从可用资源池
获得一个计算服务。nova-scheduler会根据诸如负载、内存、可用域的物理距离、CPU构架等作出调度决定。
\item 对象存储(Object):Swift，其最初是由Rackspace公司开发的高可用分布式对象存储服务，并于2010年贡献
给OpenStack开源社区作为其最初的核心子项目之一，为其Nova子项目提供虚机镜像存储服务。Swift 支持多租户模式、
容器和对象读写操作，适合解决互联网的应用场景下非结构化数据存储问题。
\item 镜像(Image):Glance,用来管理在 OpenStack 集群中的镜像，但不负责实际的存储。它为从简单文件系统到对
象存储系统(如OpenStack-Swift项目)的多种存储技术提供了一个抽象。除了实际的磁盘镜像之外，它还保存描述镜像的元数据和状态信息。
\item 身份(Identity):Keystone(OpenStack Identity Service)是OpenStack框架中，负责身份验证、服务规则和服务令牌的功能， 它实现了OpenStack的Identity API。
\item 网络地址管理(Network):Neutron是OpenStack核心项目之一，提供云计算环境下的虚拟网络功能.
\item 块存储(Block):Cinder,Cinder用来提供块存储(Block Storage)，类似于Amazon的EBS块存储服务，OpenStack中的实例是不能持久化的，需要挂载volume，在volume中实现持久化。Cinder就是提供对 volume 实际需要的存储块单元的实现管理功能。
\item UI管理界面(Dashboard):Horizon,Horizon套件提供IT人员一个图形化的网页接口，让IT人员可以综观云端服务目前的规模与状态，并且，能够统一存取、部署与管理所有云端服务所使用到的资源。
\item 测量(Metering):Ceilometer,主要负责监控数据的采集，采集的项目包括虚拟机的性能数据，n
eutron-l3-router使用的网络带宽，glance，cinder，swift等租户使用信息，
甚至是通过snmp采集物理机的信息，以及采集支持opendaylight的网络设备信息。
\item 编配(Orchestration):Heat 类似于AWS的CloudFormation, heat实现了一种自动化的通过简单定义和配置就能实现的云部署方式。
可以在heat模板中定义连串相关任务，然后交由heat，由heat按照一定的顺序执行heat模板中定义的一连串任务。利用heat还可以连接到neutron来帮助编排负载均衡和其他网络功能。
\end{itemize}

\subsection{Mesos}

Apache Mesos由四个组件组成，分别是Mesos-master，Mesos-slave，framework和executor。

Mesos-master是整个系统的核心，负责管理接入mesos的各个framework（由frameworks\_manager管理）和 slave（由slaves\_manager管理），并将slave上的资源按照某种策略分配给framework（由独立插拔模块Allocator管 理）。

Mesos-slave负责接收并执行来自mesos-master的命令、管理节点上的mesos-task，并为各个task分配资源。 mesos-slave将自己的资源量发送给mesos-master，由mesos-master中的Allocator模块决定将资源分配给哪个 framework，当前考虑的资源有CPU和内存两种，也就是说，mesos-slave会将CPU个数和内存量发送给mesos-master，而用 户提交作业时，需要指定每个任务需要的CPU个数和内存量，这样，当任务运行时，mesos-slave会将任务放到包含固定资源的linux container中运行，以达到资源隔离的效果。很明显，master存在单点故障问题，为此，mesos采用了zookeeper解决该问题。

Framework是指外部的计算框架，如Hadoop，Mesos等，这些计算框架可通过注册的方式接入mesos，以便mesos进行统一管理 和资源分配。Mesos要求可接入的框架必须有一个调度器模块，该调度器负责框架内部的任务调度。当一个framework想要接入mesos时，需要修 改自己的调度器，以便向mesos注册，并获取mesos分配给自己的资源， 这样再由自己的调度器将这些资源分配给框架中的任务，也就是说，整个mesos系统采用了双层调度框架：第一层，由mesos将资源分配给框架；第二层， 框架自己的调度器将资源分配给自己内部的任务。当前Mesos支持三种语言编写的调度器，分别是C++，java和python，为了向各种调度器提供统 一的接入方式，Mesos内部采用C++实现了一个MesosSchedulerDriver（调度器驱动器），framework的调度器可调用该 driver中的接口与Mesos-master交互，完成一系列功能（如注册，资源分配等）。

Executor主要用于启动框架内部的task。由于不同的框架，启动task的接口或者方式不同，当一个新的框架要接入mesos时，需要编写 一个executor，告诉mesos如何启动该框架中的task。为了向各种框架提供统一的执行器编写方式，Mesos内部采用C++实现了一个 MesosExecutorDiver（执行器驱动器），framework可通过该驱动器的相关接口告诉mesos启动task的方法。

\subsection{YARN}
YARN是下一代MapReduce，即MRv2，是在第一代MapReduce基础上演变而来的，主要是为了解决原始Hadoop扩展性较差，不支持多计算框架而提出的。它完全不同于Hadoop MapReduce，所有代码全部重写而成。整个平台由Resource Manager（master，功能是资源分配）和Node Manager组成（slave，功能是节点管理）。较于HadoopMapReduce，其最大特点是将JobTracker拆分成Resource Manager和Application Master，其中Resource Manager是全局的资源管理器，仅负责资源分配（由于Resource Manager功能简单，所以不会严重制约系统的扩展性），而Application Master对应一个具体的application（如Hadoop job， Spark Job等），主要负责application的资源申请，启动各个任务和运行状态监控（没有调度功能）。

$\bullet$ ResourceManager

ResourceManager作为资源的协调者有两个主要的组件：Scheduler和ApplicationsManager(AsM)。

Scheduler负责分配最少但满足application运行所需的资源量给Application。Scheduler只是基于资源的使用情况进行调度，并不负责监视/跟踪application的状态，当然也不会处理失败的task。RM使用resource container概念来管理集群的资源，resource container是资源的抽象，每个container包括一定的内存、IO、网络等资源，不过目前的实现只包括内存一种资源。

ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。

$\bullet$ NodeManager

NM主要负责启动RM分配给AM的container以及代表AM的container，并且会监视container的运行情况。在启动container的时候，NM会设置一些必要的环境变量以及将container运行所需的jar包、文件等从hdfs下载到本地，也就是所谓的资源本地化；当所有准备工作做好后，才会启动代表该container的脚本将程序启动起来。启动起来后，NM会周期性的监视该container运行占用的资源情况，若是超过了该container所声明的资源量，则会kill掉该container所代表的进程。

另外，NM还提供了一个简单的服务以管理它所在机器的本地目录。Applications可以继续访问本地目录即使那台机器上已经没有了属于它的container在运行。例如，Map-Reduce应用程序使用这个服务存储map output并且shuffle它们给相应的reduce task。

在NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的。

$\bullet$ ApplicationMaster

ApplicationMaster是一个框架特殊的库，对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，对于其他的想要运行在yarn上的计算模型而言，必须得实现针对该计算模型的ApplicationMaster用以向RM申请资源运行task，比如运行在yarn上的spark框架也有对应的ApplicationMaster实现，归根结底，yarn是一个资源管理的框架，并不是一个计算框架，要想在yarn上运行应用程序，还得有特定的计算框架的实现。由于yarn是伴随着MRv2一起出现的，所以下面简要概述MRv2在yarn上的运行流程。

\section{资源管理抽象模型}
 
资源管理调度模型如图所示

图中说明了一个资源管理与系统的抽象模型。从概念上讲资源管理与调度系统的主要目的是根据用户任务的请求，
从集群中分配资源。目前资源分配的主要资源主要包括内存,CPU,网络和IO资源等.而资源调度的模型主要涉及3个要素
资源组织模型,调度算法和任务组织方式。
\begin{enumerate}
\item 资源组织模型主要是整理和规整集群中所有的资源，以方便后续的资源分配过程。通常的做法是将资源组织成为多层级队列的形式，
例如Corona的"All-resource->group->pool"的三级队列模式组织，另外平级队列或是单队列也是常见的资源
组织模型，其本质是多层级队列的特殊形式。
\item 资源调度算法的负责将集群中的资源按照一定的策略分配的任务。常见的调度策略有FIFO调度算法、公平调度算法、能力调度算法以及延迟调度算法等。具体策略可以参加本章2.4资源调度算法。
\item 任务组织方式主要是将多用户提交的任务通过一定的方式组织起来，来方便调度算法进行分配。常见的组织方式是将任务以队列的形式组织起来，例如Hadoop1.0中将任务按照多队列组织，队列之间采用平级关系，而在hadoop2.0中的任务队列则增加了层级队列的树形结构，用以提供更加灵活的方式来管理任务队列。
\end{enumerate}
\section{资源管理与调度系统范型}
目前有多种多种的资源管理和调度系统实现，根据实际的宏观运行机制进行分类，可以分为以下几种资源管理与调度系统范型:集中式调度，两级调度器和状态共享调度器。

\subsection{集中式调度器}
集中式调度器在整个系统中只有一个唯一的全局调度器，其特点是，资源的调度和作业的管理功能全部放到一个进程中完成，在
集群上运行的所有框架或者是计算任务的资源请求均由集中式调度器来满足，因此，整个调度系统在集群上缺乏并发性，并且所有
的调度逻辑均由集中式调度器独自完成。开源界典型的代表是Hadoop JobTracker的实现。

这类调度器又分为2种类型，一种是单路径调度器,另一种是多路径调度器。
\begin{enumerate}
\item 单路径调度器(Single Path Scheduler):这种调度器是指不管任务的类型均采用全局统一的调度器进行资源管理，
这种调度算法基本是采用融合考虑各种因素来实现的调度器,在此基础之上结合任务的优先级，集群资源的状态统计，决定调度的
任务资源分配和调度顺序。
\item 多路径调度器(Multi Path scheduler):这种调度器在单路经调度器的基础之上做了该进,首先支持多种调度策略,其大致思路
是将调度算法模块化,根据任务的类型进行调度,比如批处理的任务采用批处理的调度算法,计算量大的任务采用计算量大的调度
算法,在具体的实现的时候各个算法独自实现,而在调度器的逻辑中根据任务类型进行选择,其选择类型类似于结构化程序编程的switch
-case分支路径,这种调度器可以根据调度算法的类型采用多线程的方式进行实现,较单路经调度器的调度灵活性和
并发性有了一定的提高。
\end{enumerate}

但是集中式调度器的设计方式的缺点也是比较明显的。
\begin{enumerate}
\item 由于将所有的调度逻辑均写入中央调度器中，所以实现的逻辑比较负载，可扩展较差。这点在单路径调度器上尤其明显，多
路径调度器虽然在此基础之上进行了改进，但是在针对某个类型任务的算法进行替换的过程中，中央调度器此时不得不整体停止，
对其他的调度任务产生影响。
\item 集群的规模受限，中央调度器的并发性不足.由于是对整个系统的全局资源进行调度，在小规模集群上可以完成其调度工
作而当集群规模扩大时，任务和工作负载加大，当调度与资源分配决策执行时间较长的情况下，这个时候往往调度器会首先达到
工作饱和，此时，后续任务会花费大量的时间在等待被调度，从而导致集中式调度器成为整个系统的运行瓶颈，严重影响系统的运
行性能。
\end{enumerate}

\subsection{两层调度器}
为了解决中央式调度器的不足，双层调度器是一种很容易想到的解决之。两层调度器将整个系统的调度工作分成两个级别：
中央调度器和框架调度器。中央调度器负责集群系统中所有资源的管理，并按照一定的策略将集群中所有的资源分配给各个计算
框架，从总体上看，中央调度器是一种比较粗粒度的资源调度，各个计算框架在接受到资源的分配后，根据自身计算任务的需要
再次按照框架调度器调度进行任务执行的调度。在这种情况下，中央调度器进行资源的切分，而框架调度器重点则在任务的调度
上，框架调度器进一步细粒度的使用中央调度器所划分的资源，本质上是分而治之策略或者是策略下放机制。Mesos，YARN是
典型的两层调度的实现。

在Mesos中资源管理部分由两部分组成：分别是Mesos Master和Mesos Slave，其中，Mesos Slave是每个节点上的代理，
负责向Master汇报信息和接收并执行来自Master的命令，而Master则是一个轻量级中央化的资源管理器，负责管理和
分配整个集群中的资源。如果一个应用程序想通过Mesos资源管理系统申请和使用资源，需编写两个组件：框架调度
器和框架执行器，其中，框架调度器负责从Mesos Master上获取资源、将资源分配给自己内部的各个应用程序，并控
制应用程序的执行过程；而框架执行器运行在Mesos Slave中，负责运行该框架中的任务。当前很多框架可以接入Mesos中
，包括Hadoop、MPI、Spark等。

Mesos Master仅将可用的资源推送给各个框架，而框架自己选择使用还是拒绝这些资源；
一旦框架（比如Hadoop JobTracker）接收到新资源后，再进一步将资源分配给其内部的各个应用程
序（各个MapReduce作业），进而实现双层调度。

与集中式调度器相比，双层调度器由于存在框架调度器，各个框架调度器仅仅与中央调度器进行耦合，所以具有良好的并发性，系
统的整体性能较佳，也比较适合大规模的多任务高负载的计算任务，
各个框架调度器并不知道整个集群资源使用情况，只是被动的接收资源；
双层调度器的缺点是：
1）  各个框架无法知道整个集群的实时资源使用情况。
很多框架不需要知道整个集群的实时资源使用情况就可以运行的很顺畅，但是对于其他一些应用，为之提供实时资源使
用情况可以为之提供潜在的优化空间，比如，当集群非常繁忙时，一个服务失败了，是选择换一个节点重新运行它呢，
还是继续在这个节点上运行？通常而言，换一个节点可能会更有利，但是，如果此时集群非常繁忙，所有节点只剩下小
于5GB的内存，而这个服务需要10GB内存，那么换一个节点可能意味着长时间等待资源释放，而这个等待时间是无法
确定的。
2）  采用悲观锁，并发粒度小。
在数据库领域，悲观锁与乐观锁争论一直不休，悲观锁通常采用锁机制控制并发，这会大大降低性能，而乐观锁则采用
多版本并发控制(MVCC ,Multi-Version Concurrency Control)，典型代表是MySQL innoDB，这种机制通过多版本方式控
制并发，可大大提升性能。在Mesos中，在任意一个时刻，Mesos资源调度器只会将所有资源推送给任意一个框架，等
到该框架返回资源使用情况后，才能够将资源推动给其他框架，因此，Mesos资源调度器中实际上有一个全局锁，这大
大限制了系统并发性。
\subsection{状态共享调度器}
 状态共享调度器是Google的Omega调度系统提出的一种资源管理与调度范型。在这种调度范型里面，各个计算框架可以
 看到整个集群的所有的资源的状态。集群之间采用互相竞争的方式去获得自己所需的资源，根据自身特性采取不同的调度
 策略。为了能够克服双层调度器的缺点，状态共享调度器将双层调度器中的集中式资源调度模块简化成了一些持久化的共
 享数据（状态）和针对这些数据的验证代码，而这里的“共享数据”实际上就是整个集群的实时资源使用信息。一旦引入共
 享数据后，共享数据的并发访问方式就成为该系统设计的核心，而Omega则采用了传统数据库中基于多版本的并发访问控制方式
（也称为“乐观锁”, MVCC, Multi-Version Concurrency Control），这大大提升了Omega的并发性。

与两层调度器不同呢，由于Omega不再有集中式的调度模块，因此，不能像Mesos或者YARN那样，在一个统一模块中完
成以下功能：对整个集群中的所有资源分组，限制每类应用程序的资源使用量，限制每个用户的资源使用量等，Omega仅
仅维护一本可恢复的集群资源状态的主信息副本。而每个计算框架维护一份私有的副本信息，这些副本的信息由各个框架
调度器自我管理和控制，框架对系统的资源需求和直接在该副本上进行；只要框架的优先级足够，就可以在该副本上申请
相应的资源获释抢夺已经分配给其他计算框架的资源，一旦框架做出决定，则该副本的资源信息将通过事务的方式同步到
全局的状态共享副本中去，从而保障操作的原子性。

由于采用了MVCC的并发访问控制策略，并且支持优先级的调度过程，框架之间会产生冲突和竞争，在多个框架申请同一
份资源时，优先级最高的那个应用程序将获得该资源，其他资源限制全部下放到各个子调度器。这个过程可以看出，Omega
是一种以效率为优先的调度器设计，由于高优先级的抢占的存在，往往使得低优先级的任务得不到满足，可能会产生饿死的
可能。从此可以看出，如果资源访问冲突的次数，冲突次数越多，系统性能下降的越快，而google通过实际负载测试证明，
在现实中往往在线的计算任务往往较多，并且实时性要求较高，具有较高的优先级，而后台的这种批处理，统计的作业较少
，两种作业的异质性较高，因而这种冲突的方式和冲突次数是完全可以接受的。

上述简述了3种不同的资源管理与调度范型，从集中式调度器到两层调度器再到状态共享状态调度器的发展过程来看，这是
一个逐步弱化中央调度器，逐渐增强框架调度器的过程。在提高调度并发性的同时，我们可以看出不同调度范型适合的应用
场景启示并不尽相同。集中式调度器比较适合小规模集群下的资源管理，而两层调度器比较适合负载同质的大规模集群应用
场景，而状态共享调度器则比较适合异质性并且冲突不高的大规模集群应用。

另外，从不同调度范型的发胀过程可以总结出，大规模数据处理系统的一个趋势是在硬件层上建立一个统一的资源管理调度
系统，而这个资源调度管理系统应该是由一个提供弱服务的中央的中央调度器和多个自由度较高的框架调度器组成，以增强
集群的硬件利用率，集群调度的并发性，可扩展性和调度的灵活性。

\section{资源调度算法}
 在多用户作业的环境下，无论中央调度器还是框架调度器都要涉及资源分配和任务分发的策略和算法。本节主要讨论常见的
 资源调度算法和任务分配策略，其中Hadoop已经将FIFO，公平调度器和能力调度器集中进去，另外，本节还将介绍一种常
 见的优化任务策略。
\subsection{FIFO调度算法}
FIFO策略是最简单的调度策略，提交的作业按照提交时间的先后顺序获释根据优先级将其放入线性队列之中，在资源调度的时候
按照次序取一个或者多个按照先进先出的顺训进行资源进行调度。这种调度算法比较常见于任务队列或是集中式调度的器的实现，
由于整个集群中仅仅存在一个队列，按照队列先后顺训进行资源分配，当集群的规模较大，这个调度器对新提交的作业的处理往往
不是及时的，会造成短作业的任务的饿死的情况。
\subsection{公平调度策略}
公平调度是一种赋予作业（job）资源的方法，它的目的是让所有的作业随着时间的推移，都能平均的获取等同的共享资源。
当单独一个作业在运行时，它将使用整个集群。当有其它作业被提交上来时，系统会将任务（task）空闲时间片（slot）赋给这些新的作业，
以使得每一个作业都大概获取到等量的CPU时间。与Hadoop默认调度器维护一个作业队列不同，这个特性让小作业在合理的时间内完成的同时又不“饿”到消耗较长时间的大作业。
它也是一个在多用户间共享集群的简单方法。公平共享可以和作业优先权搭配使用——优先权像权重一样用作为决定每个作业所能获取的整体计算时间的比例。

公平调度器按资源池（pool）来组织作业，并把资源公平的分到这些资源池里。默认情况下，每一个用户拥有一个独立的资源池，
以使每个用户都能获得一份等同的集群资源而不管他们提交了多少作业。按用户的Unix群组或作业配置（jobconf）属性来设置作业的资源池也是可以的。
在每一个资源池内，会使用公平共享（fairsharing）的方法在运行作业之间共享容量（capacity）。

除了提供公平共享方法外，公平调度器允许赋给资源池保证（guaranteed）最小资源保障量，这个用在确保特定用户、群组或生产应用程序总能获取到足够的资源时是很有用的。
当一个资源池包含作业时，它至少能获取到它的最小共享资源，但是当资源池不完全需要它所拥有的保证共享资源时，额外的部分会在其它资源池间进行切分。

公平调度器是Facebook为Hadoop按照公平调度开发的多用户多作业调度策略，公平调度策略在Hadoop的实现已经分为两层，已初步具有两层
调度的雏形。第一层将集群资源公平地分配到资源池（pool）中（每一个资源池对应一个用户），第二层在pool中将资源分配给用户的作业。
其调度的过程大致为以下三步：
\begin{enumerate}
\item 根据每个资源池的最小资源保障量，将系统中的部分资源分配给各个资源池。
\item 根据资源池的优先级将剩余资源按照比例分配给各个资源池。
\item 各个资源池中，自己按照优先级组织自己的队列。
\end{enumerate}
　　
在常规操作中，当提交了一个新作业时，公平调度器会等待已运行作业中的任务完成以释放时间片给新的作业。
但公平调度器也支持在可配置的超时时间后对运行中的作业进行抢占
。如果新的作业在一定时间内还获取不到最小的共享资源，这个作业被允许去终结已运行作业中的任务以获取运行所需要的资源。
因此抢占可以用来保证“生产”作业在指定时间内运行的同时也让Hadoop集群能被实验或研究作业使用。
另外，作业的资源在可配置的超时时间（一般设置大于最小共享资源超时时间）内拥有不到其公平共享资源（fair share）的一半的时候也允许对任务进行抢占。
在选择需要结束的任务时，公平调度器会在所有作业中选择那些最近运行起来的任务，以最小化被浪费的计算。
抢占不会导致被抢占的作业失败，因为Hadoop作业能容忍丢失任务，这只是会让它们的运行时间更长。
\subsection{能力调度器}
可以看作是FifoScheduler的多队列版本。每个队列可以限制资源使用量。
但是，队列间的资源分配以使用量作排列依据，使得容量小的队列有竞争优势。
集群整体吞吐较大。延迟调度机制使得应用可以放弃，夸机器或者夸机架的调度机会，争取本地调度。

能力调度器以队列为单位划分资源，每个队列都有资源使用的下限和上限。每个用户也可以设定资源使用上限。
在一个作业调度中，如果某个队列的资源有剩余，该列的剩余资源可以共享给另一个队列，其他队列使用后还可以归还。
管理员可以约束单个队列、用户或作业的资源使用。支持资源密集型作业，可以给某些作业分配多个slot（这是比较特殊的一点）。
支持作业优先级，但不支持资源抢占。能力调度的方法更强调的是用户之间的公平而非作业之间的公平
\subsection{延迟调度策略}
延迟调度严格的来说并不是一个调度算法，延迟调度策略往往会作为其他策略增加调度的数据局部性，以此来增加任务的执行效率。

其基本的的思想如下所示：

对于当前被调度的要被分配的任务，如果当前的资源不满足数据的局部性，那么可以暂时放弃分配公平性，任务不接受当前资源，而是等待后续的资源分配
当前资源可以跳过该任务分配给其他待调度的任务，如果该任务被跳过的次数超过阈值扔无法满足数据的局部性，那么放弃数据局部性，接受当前的资源运行该任务。

延迟调度的很典型的应用在Hadoop的应用上。
Hadoop是构建在以hdfs为基础的文件系统之上的。YARN所运行的应用的绝大部分输入都是hdfs上的文件。而hdfs上的文件的是分块多副本存储的。
假设文件系统的备份因子是3。则每一个文件块都会在3个机器上有副本。在YARN运行应用的时候，AM会将输入文件进行切割
，然后，AM向RM申请的container来运行task来处理这些被切割的文件段。

假如输入文件在ABC三个机器上有备份，那如果AM申请到的container在这3个机器上的其中一个，
那这个task就无须从其它机器上传输要处理的文件段，节省网络传输。这就是Hadoop的本地优化。
所以Hadoop的文件备份数量除了和数据安全有关，还对应用运行效率有莫大关系。

YARN的实现本地优化的方式是AM给RM提交的资源申请的时候，会同时发送本地申请，机架申请和任意申请。
然后，RM的匹配这些资源申请的时候，会先匹配本地申请，再匹配机架申请，最后才匹配任意申请。

\section{本章小结}
本章主要详细的介绍了云计算中的平台，云计算资源调度器的设计思路，设计范型等，同时给出了常见的资源调度的算法和策略。
